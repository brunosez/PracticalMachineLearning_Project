<html>
<body>
<p>Project for Practical Machine Learning course</p>
<p>We comment ou R program, which is located in the same repo as</p>
<p>We put it as a function, to be able to use arguments to choose the training file. In fact a simple script is sufficent.</p>
<p>We choose to suppress some variable tha where computed from other. here is the code used to achieve this suppression : ’’’suppressed_features&lt;-grep(“var_<em>|avg_</em>|stddev_<em>|max_</em>|<em>amplitude_</em>|min_<em>|skewness_</em>|kurtosis_*“, names(projtrain)) # this command will suppress 100 variables in the dataset. projtrain2&lt;-projtrain[,-suppressed_features]‘’’</p>
<p>Same for the test set.</p>
<p>We create a Data Partition in order to minimize the volume of data. In fact as we use cross validation later this data partition is useless.</p>
<p>We use repeated cross validation to train our Random Forest algorithm.</p>
<p>We use the parallel library Snow to parallelize the computation which takes around 40-50’. ‘’’cluster &lt;- makeCluster(2) registerDoSNOW(cluster)’’’</p>
<p>Our Random Forest algo is limited to 100 trees. THe code used is ‘’’modFit&lt;-train(classe ~ ., data=projtrain2,method=“rf”, trControl= ctrl,prox=TRUE,metric=“ROC”,ntree=100)’’’ Finally we do some classical confusion matrix computation.</p>
</body>
</html>
